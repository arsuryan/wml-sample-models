{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import a Spark MLlib model into IBM Watson Machine Learning\n",
    "\n",
    "Importing a model into Watson Machine Learning means to store a trained model in your Watson Machine Learning repository and then deploy the stored model.  This notebook demonstrates importing a Spark MLlib PipelineModel object.\n",
    "\n",
    "See also: <a href=\"https://dataplatform.cloud.ibm.com/docs/content/analyze-data/ml-import-spark-mllib.html\" target=\"_blank\" rel=\"noopener noreferrer\">Importing a Spark MLlib model</a>\n",
    "\n",
    "This notebook runs on Spark Python 3.5.\n",
    "\n",
    "\n",
    "### Notebook sections\n",
    "\n",
    "[Step 0: Build and train a model, and then save the model and training data](#step0)\n",
    "\n",
    "[Step 1: Store the model in your Watson Machine Learning repository](#step1)\n",
    "\n",
    "[Step 2: Deploy the stored modelin your Watson Machine Learning service](#step2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"step0\"></a> Step 0: Build, train, and save a model\n",
    "\n",
    "**About the sample model**\n",
    "\n",
    "The sample model built here is a logistic regression model for predicting whether or not a customer will purchase a tent from a fictional outdoor equipment store, based on the customer charateristics.\n",
    "\n",
    "The data used to train the model is the \"GoSales.csv\" training data in the IBM Watson Studio community: <a href=\"https://dataplatform.cloud.ibm.com/exchange/public/entry/view/aa07a773f71cf1172a349f33e2028e4e\" target=\"_blank\" rel=\"noopener noreferrer\">GoSales sample data</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get and prepare training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wget # needed to download sample training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GoSales (1).csv\n"
     ]
    }
   ],
   "source": [
    "# Download sample training data to notebook working directory\n",
    "import wget\n",
    "training_data_url = 'https://dataplatform.cloud.ibm.com/data/exchange-api/v1/entries/aa07a773f71cf1172a349f33e2028e4e/data?accessKey=e98b7315f84e5448aa94c633ca66ea83'\n",
    "filename = wget.download( training_data_url )\n",
    "print( filename )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- GENDER: string (nullable = true)\n",
      " |-- AGE: integer (nullable = true)\n",
      " |-- MARITAL_STATUS: string (nullable = true)\n",
      " |-- PROFESSION: string (nullable = true)\n",
      " |-- IS_TENT: boolean (nullable = true)\n",
      " |-- PRODUCT_LINE: string (nullable = true)\n",
      " |-- PURCHASE_AMOUNT: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read sample data into a Spark DataFrame\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "df = spark.read\\\n",
    "  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n",
    "  .option('header', 'true')\\\n",
    "  .option('inferSchema', 'true')\\\n",
    "  .load( filename )\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- GENDER: string (nullable = true)\n",
      " |-- AGE: integer (nullable = true)\n",
      " |-- MARITAL_STATUS: string (nullable = true)\n",
      " |-- PROFESSION: string (nullable = true)\n",
      " |-- IS_TENT: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select columns of interest\n",
    "from pyspark.sql.types import IntegerType\n",
    "training_data = df.select( \"GENDER\", \"AGE\", \"MARITAL_STATUS\", \"PROFESSION\", df.IS_TENT.cast( IntegerType() ) )\n",
    "training_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a PipelineModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create indexers for string columns\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "indexer_GENDER         = StringIndexer( inputCol=\"GENDER\",         outputCol=\"GENDER_index\"         )\n",
    "indexer_MARITAL_STATUS = StringIndexer( inputCol=\"MARITAL_STATUS\", outputCol=\"MARITAL_STATUS_index\" )\n",
    "indexer_PROFESSION     = StringIndexer( inputCol=\"PROFESSION\",     outputCol=\"PROFESSION_index\"     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an assembler that generates the feature vector column\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "feature_vector_assembler = VectorAssembler( inputCols=[ \"GENDER_index\", \"AGE\", \"MARITAL_STATUS_index\", \"PROFESSION_index\" ],  outputCol=\"features\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a logistic regression model\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression( featuresCol='features', labelCol='IS_TENT' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PipelineModel\n",
    "from pyspark.ml import Pipeline\n",
    "pipeline_org = Pipeline( stages=[ indexer_GENDER, indexer_MARITAL_STATUS, indexer_PROFESSION, feature_vector_assembler, lr ] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train count: 45186\n",
      "Test count: 15066\n"
     ]
    }
   ],
   "source": [
    "# Split the training data into a training set and a test set\n",
    "train_org, test = training_data.randomSplit( [ 0.75, 0.25 ], seed = 2019 )\n",
    "print( \"Train count: \" + str( train_org.count() ) )\n",
    "print( \"Test count: \"  + str( test.count()  ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the PipelineModel\n",
    "pipeline_model_org = pipeline.fit( train_org )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success rate: 78%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model performance\n",
    "predictions_org = pipeline_model_org.transform( test )\n",
    "correct_false_org = predictions_org.filter( \"IS_TENT == 0 AND prediction == 0.0\" )\n",
    "correct_true_org = predictions_org.filter( \"IS_TENT == 1 AND prediction != 0.0\" )\n",
    "print( \"Success rate: \" + str( round( 100 * ( ( correct_false_org.count() + correct_true_org.count() ) / predictions_org.count() ) ) ) + \"%\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the training data, the Pipeline, and the PipelineModel\n",
    "\n",
    "To import your Spark MLlib PipeineModel into Watson Machine Learning, you need three things:\n",
    "- The training set (Spark DataFrame)\n",
    "- The PipelineModel object\n",
    "- The Pipeline object (which you can get from the PipelineModel object)\n",
    "\n",
    "In this section of the notebook, the training data and the PipelineModel object are saved to the notebook working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_model_org.save( \"tent-prediction-model\" )\n",
    "train_org.write.save( \"training-data.parquet\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tent-prediction-model:\n",
      "total 8\n",
      "drwxr-xr-x 2 spark spark 4096 Feb 26 19:43 metadata\n",
      "drwxr-xr-x 7 spark spark 4096 Feb 26 19:43 stages\n",
      "\n",
      "tent-prediction-model/metadata:\n",
      "total 4\n",
      "-rw-r--r-- 1 spark spark   0 Feb 26 19:43 _SUCCESS\n",
      "-rw-r--r-- 1 spark spark 357 Feb 26 19:43 part-00000\n",
      "\n",
      "tent-prediction-model/stages:\n",
      "total 20\n",
      "drwxr-xr-x 4 spark spark 4096 Feb 26 19:43 0_StringIndexer_4feba54772c2c0d34b7e\n",
      "drwxr-xr-x 4 spark spark 4096 Feb 26 19:43 1_StringIndexer_47dfa36cc7a7491a90e5\n",
      "drwxr-xr-x 4 spark spark 4096 Feb 26 19:43 2_StringIndexer_45a280a3ea48946c1033\n",
      "drwxr-xr-x 3 spark spark 4096 Feb 26 19:43 3_VectorAssembler_4f7b9d9845f504d26247\n",
      "drwxr-xr-x 4 spark spark 4096 Feb 26 19:43 4_LogisticRegression_46eaabcb01ad31bfa677\n"
     ]
    }
   ],
   "source": [
    "# Just for interest, see what is in the directory\n",
    "print( \"tent-prediction-model:\" )\n",
    "!ls -l tent-prediction-model\n",
    "print( \"\\ntent-prediction-model/metadata:\" )\n",
    "!ls -l tent-prediction-model/metadata\n",
    "print( \"\\ntent-prediction-model/stages:\" )\n",
    "!ls -l tent-prediction-model/stages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"step1\"></a> Step 1: Store the model in your Watson Machine Learning repository\n",
    "\n",
    "This section of the notebook demonstrates calling the <a href=\"https://wml-api-pyclient.mybluemix.net/index.html?highlight=store_model#client.Repository.store_model\" target=\"_blank\" rel=\"noopener noreferrer\">store_model</a> function, passing the in-memory PipelineMoel object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model into memory\n",
    "from pyspark.ml import PipelineModel\n",
    "pipeline_model = PipelineModel.load( \"tent-prediction-model\" )\n",
    "pipeline = Pipeline( stages = pipeline_model_org.stages )\n",
    "train = spark.read.load( \"training-data.parquet\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install watson_machine_learning_client # Needed to work with the Watson Machine Learning Python client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paste your Watson Machine Learning credentials in the following cell.\n",
    "\n",
    "See: <a href=\"https://dataplatform.cloud.ibm.com/docs/content/analyze-data/ml-get-wml-credentials.html\" target=\"_blank\" rel=\"noopener noreferrer\">Looking up credentials</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Watson Machine Learning client instance\n",
    "from watson_machine_learning_client import WatsonMachineLearningAPIClient\n",
    "wml_credentials = {\n",
    "    \"instance_id\": \"\",\n",
    "    \"password\": \"\",\n",
    "    \"url\": \"\",\n",
    "    \"username\": \"\"\n",
    "}\n",
    "client = WatsonMachineLearningAPIClient( wml_credentials )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the PipelineModel in the Watson Machine Learning repository\n",
    "model_details = client.repository.store_model( pipeline_model, 'Spark MLlib model', training_data=train, pipeline=pipeline )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"step2\"></a> Step 2: Deploy the stored model in your Watson Machine Learning service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "#######################################################################################\n",
      "\n",
      "Synchronous deployment creation for uid: '87d108a2-ef70-4c15-844a-b6576ee5ca43' started\n",
      "\n",
      "#######################################################################################\n",
      "\n",
      "\n",
      "INITIALIZING\n",
      "DEPLOY_IN_PROGRESS.........\n",
      "DEPLOY_SUCCESS\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------\n",
      "Successfully finished deployment creation, deployment_uid='603efbe3-0e1a-4fa1-8af7-9d2ebf2a91e7'\n",
      "------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Deploy the stored model as an online web service deployment\n",
    "model_id = model_details[\"metadata\"][\"guid\"]\n",
    "deployment_details = client.deployments.create( artifact_uid=model_id, name=\"Spark MLlib model deployment\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fields': ['GENDER',\n",
       "  'AGE',\n",
       "  'MARITAL_STATUS',\n",
       "  'PROFESSION',\n",
       "  'GENDER_index',\n",
       "  'MARITAL_STATUS_index',\n",
       "  'PROFESSION_index',\n",
       "  'features',\n",
       "  'rawPrediction',\n",
       "  'probability',\n",
       "  'prediction'],\n",
       " 'values': [['M',\n",
       "   27,\n",
       "   'Single',\n",
       "   'Professional',\n",
       "   0.0,\n",
       "   1.0,\n",
       "   1.0,\n",
       "   [0.0, 27.0, 1.0, 1.0],\n",
       "   [0.16773330636208073, -0.16773330636208073],\n",
       "   [0.541835288108435, 0.458164711891565],\n",
       "   0.0]]}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the deployment\n",
    "model_endpoint_url = client.deployments.get_scoring_url( deployment_details )\n",
    "payload = { \"fields\" : [ \"GENDER\", \"AGE\", \"MARITAL_STATUS\", \"PROFESSION\" ], \"values\" : [ [ \"M\", 27, \"Single\", \"Professional\" ] ] }\n",
    "client.deployments.score( model_endpoint_url, payload )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+--------------+------------+------------+--------------------+----------------+------------------+--------------------+--------------------+----------+\n",
      "|GENDER|AGE|MARITAL_STATUS|  PROFESSION|GENDER_index|MARITAL_STATUS_index|PROFESSION_index|          features|       rawPrediction|         probability|prediction|\n",
      "+------+---+--------------+------------+------------+--------------------+----------------+------------------+--------------------+--------------------+----------+\n",
      "|     M| 27|        Single|Professional|         0.0|                 1.0|             1.0|[0.0,27.0,1.0,1.0]|[0.16773330636208...|[0.54183528810843...|       0.0|\n",
      "+------+---+--------------+------------+------------+--------------------+----------------+------------------+--------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Testing the model locally gets the same results\n",
    "test_df = spark.createDataFrame( [ ( \"M\", 27, \"Single\", \"Professional\" ) ], [ \"GENDER\", \"AGE\", \"MARITAL_STATUS\", \"PROFESSION\" ] )\n",
    "pipeline_model.transform( test_df ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "In this notebook, you imported a Spark MLlib PipelineModel into Watson Machine Learning using the Watson Machine Learning Python client."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"authors\"></a>Authors\n",
    "\n",
    "**Sarah Packowski** is a member of the IBM Watson Studio Content Design team in Canada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<hr>\n",
    "Copyright &copy; IBM Corp. 2019. This notebook and its source code are released under the terms of the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#F5F7FA; height:110px; padding: 2em; font-size:14px;\">\n",
    "<span style=\"font-size:18px;color:#152935;\">Love this notebook? </span>\n",
    "<span style=\"font-size:15px;color:#152935;float:right;margin-right:40px;\">Don't have an account yet?</span><br>\n",
    "<span style=\"color:#5A6872;\">Share it with your colleagues and help them discover the power of Watson Studio!</span>\n",
    "<span style=\"border: 1px solid #3d70b2;padding:8px;float:right;margin-right:40px; color:#3d70b2;\"><a href=\"https://ibm.co/wsnotebooks\" target=\"_blank\" style=\"color: #3d70b2;text-decoration: none;\">Sign Up</a></span><br>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5 with Spark",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
